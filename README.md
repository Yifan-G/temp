Project Description
** DocTalk performs summarization and keyphrase extraction. Doctalk downloads stanza_corenlp and nltk_data automatically , please make sure that there is internet access.

The Directories
EvalCraft # The ROUGE evaluator that calls doctalk to process documents, calculating ROUGE score
EvalCraft/doctalk/ # Our graph-based keyphrase extraction and summarization model
EvalCraft/dataset/ # Contains sample documents from various datasets we used in the paper.
Datasets:
Under the directory EvalCraft/dataset/, there are several subdirectories that each contain sample files from a dataset. NOTE: In the interest of decreasing the document's size to enable easy upload, each dataset directory contains only about 5 documents from the dataset.

## Setup enviroment in linux:
- python 3.6 or newer, pip3, java 9.x or newer, SWI-Prolog 8.x or newer, graphviz
- also, having git installed is recommended for easy updates
The steps are as below with root:
$ yum install epel-release
$ yum install python3-pip
$ python3 -m pip install --upgrade pip setuptools wheel
$ pip3 install virtualenv
$ virtualenv -p python3.6 venv
$ . venv/bin/activate

install java
    $ yum -y install java

install  EvalCraft dependent packages, in  EvalCraft folder
  $ pip install -r requirements.txt

if doctalk is used:
    copy doctalk requirements.txt under doctalk folder
    cd to  doctalk folder
    $ pip install -r requirements.txt

 if  StanzaGraphs is used, go to StanzaGraphs folder, run
    $ pip install -r requirements.txt
 
check packages version be command
    $ pip freeze 
run  every access
    $ . venv/bin/activate


EvalCraft/dataset/:

Inspec # Inspec dataset, used for keyphrase extraction
Krapivin_Full # Full documents from the Krapivin dataset, used by us for both keyphrase extraction and summarization.
Krapivin_NoAbstract # Abstract-removed documents from the Krapivin dataset, used by us for both keyphrase extraction and summarization.
NUS_Full # Full documents from the NUS dataset, used by us for both keyphrase extraction and summarization.
NUS_NoAbstract # Abstract-removed documents from the NUS dataset, used by us for both keyphrase extraction and summarization.
SemEval_Full # Full documents from the SemEval dataset, used by us for both keyphrase extraction and summarization
SemEval_NoAbstract # Abstract-removed documents from the SemEval dataset, used by us for both keyphrase extraction and summarization
PubMed # PubMed dataset, for summarization
arXiv # arXiv dataset, for summarization
each dataset directory (ex. EvalCraft/dataset/Inspec/) has the subdirectories:

abs # Gold-standard summaries
docsutf8 # original article
keys # Gold-standard keyphrases
temp_docs # json files generated by stanza_corenlp parser
out # Directory containing Keyphrases and summaries generated by DocTalk
out/keys # Keyphrases generated by DocTalk
out/abs # Summaries generated by DocTalk
If you'd like to do full tests on datasets, you can put all articles from a dataset into docsutf8 and put the gold standard keyphrases and summaries into keys and abs respectively.
The datasets our model was tested on are available at:

arXiv and PubMed: https://github.com/armancohan/long-summarization Krapivin, SemEval, NUS, Inspec: https://github.com/snkim/AutomaticKeyphraseExtraction

Run eval_sumkeys Python Files to Evaluate DocTalk's performance on Keyphrase Extraction and Summarization
There are several files with names that start with eval_sumkeys-, each file corresponds to one type of dataset we tested our model on. By directly running one of the files, ROUGE scores will be calculated and outputted for that test.

eval_sumkeys-Inspec.py : Inspec dataset
eval_sumkeys-Krapivin_Full.py : Krapivin dataset with full documents
eval_sumkeys-Krapivin_NoAbstract.py : Krapivin dataset with abstract-removed documents
eval_sumkeys-NUS_Full.py : NUS dataset with full documents
eval_sumkeys-NUS_NoAbstract.py : NUS dataset with abstract-removed documents
eval_sumkeys-SemEval_Full.py : SemEval dataset with full documents
eval_sumkeys-SemEval_NoAbstract.py : SemEval dataset with abstract-removed documents
eval_sumkeys-PubMed.py : PubMed dataset
eval_sumkeys-arXiv.py : arXiv dataset
A sample output is:

  EXTRACTED KEYS AND ABSTRACTS
                  Precision,          Recall,           F-Measure
  KEYS ROUGE 1 : 0.29827969008116073 0.4364484126984127 0.34144388195074693
  ABS ROUGE 1 : 0.49158123673586773    0.5043105876111166    0.48753007181944447
  ABS ROUGE 2 : 0.2825046451122882    0.29587238206313704    0.28307249204689017
  ABS ROUGE l : 0.5404513073395705    0.5515895825486291    0.537938640371076
  ABS ROUGE w : 0.27062101520754644    0.1830950391544452    0.21442998206315905
  DONE
  SYSTEM : DOCTALK
  wk 10 sk 8
  with_full_text =  False
  max_docs =  0
  force =  1
Parameters: in eval_sumkeys-* files, such as eval_sumkeys-Krapivin_Full.py - # 2 = forces deletion of json in temp_docs, 1 = forces deletion of keys+abs in out, 0 = deletes nothing force=0 - # number of keyphrases and summary sentences wk,sk=10,8 # wk for number of keyphrases, sk for number of summary sentences - # sets maximum number of documents to be processed, all documents will be processed if set to 0 max_docs = 0